# For more detailed information, consult the documentation at:
# https://docs.litellm.ai/docs/proxy/configs

model_list:                                                         # List of model configurations
  - model_name: gpt-3.5-turbo-instruct                              # A user-defined name for the model configuration
    litellm_params:                                                 # Parameters specific to LiteLLM for this model
        model: text-completion-openai/gpt-3.5-turbo-instruct        # The specific model identifier for GPT-3.5-turbo-instruct
        api_key: os.environ/OPENAI_API_KEY                          # Reference to the API key stored in the environment variable OPENAI_API_KEY
        max_tokens: 100                                             # Maximum number of tokens allowed in the model's response
        temperature: 0.5                                            # Controls the randomness of the output (0: deterministic, 1: highly random)
